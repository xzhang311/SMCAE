function [cost,grad] = sparseAutoencoderPredict_balanced_simplified(theta, visibleSize, hiddenSize, ...
                                             lambda, sparsityParam, balanceParam, beta, ldata_in, rdata_in)

% visibleSize: the number of input units (probably 64) 
% hiddenSize: the number of hidden units (probably 25) 
% lambda: weight decay parameter
% sparsityParam: The desired average activation for the hidden units (denoted in the lecture
%                           notes by the greek alphabet rho, which looks like a lower-case "p").
% beta: weight of sparsity penalty term
% data: Our 64x10000 matrix containing the training data.  So, data(:,i) is the i-th training example. 
  
% The input theta is a vector (because minFunc expects the parameters to be a vector). 
% We first convert theta to the (W1, W2, b1, b2) matrix/vector format, so that this 
% follows the notation convention of the lecture notes. 

% Structure of theta:
% _______________________________________________
% |___W1___|___lW2___|___rW2___|_b1_|_lb2_|_rb2_|

wLen = hiddenSize*visibleSize;
bLen1 =  hiddenSize;
bLen2 =  visibleSize;

W1 = reshape(theta(1 : wLen), hiddenSize, visibleSize);
lW2 = reshape(theta(wLen+1 : 2*wLen), visibleSize, hiddenSize);
rW2 = reshape(theta(2*wLen+1 : 3*wLen), visibleSize, hiddenSize);
b1 = theta(3*wLen+1 : 3*wLen+bLen1);
lb2 = theta(3*wLen+bLen1+1 : 3*wLen+bLen1+bLen2);
rb2 = theta(3*wLen+bLen1+bLen2+1 : end);

[ndims, lm] = size(ldata_in);
[ndims, rm] = size(rdata_in);

% forward pass for left branch
lz2 = W1 * ldata_in + repmat(b1, 1, lm);
la2 = sigmoid(lz2);
lz3 = lW2 * la2 + repmat(lb2, 1, lm);
la3 = sigmoid(lz3);

% forward pass for right branch
rz2 = W1 * rdata_in + repmat(b1, 1, rm);
ra2 = sigmoid(rz2);
rz3 = rW2 * ra2 + repmat(rb2, 1, rm);
ra3 = sigmoid(rz3);
end

function sigm = sigmoid(x)
    sigm = 1 ./ (1 + exp(-x));
end

function ans = kl(r, rh)
    ans = sum(r .* log(r ./ rh) + (1-r) .* log( (1-r) ./ (1-rh)));
end

function ans = kl_delta(r, rh)
    ans = -(r./rh) + (1-r) ./ (1-rh);
end

